{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ABOVE | AC9 Data Processing\n",
    "***\n",
    "## Step 01 Read AC9 files\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Authors:** Catherine Kuhn and Elena TerziÄ‡ and Anna Simpson\n",
    "**Last Updated:** September 17th, 2018\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code reads in raw ac9 .dat files and parses sample information from the filename and header information inside the file. The output is a table of summary statistics formatted as a *.csv* file for each wavelength for each file. This code was built for a worflow in which a and c sides are sampled separately. File names should contain: date, site, rep, a or c side and water temperature. \n",
    "\n",
    "File names are formatted like ** AC9_dddddd_sit_sam_s_r_TXX_XX.dat** where:\n",
    "\n",
    "- **dddddd** = date (071718)\n",
    "- **sit** = three letter site code (fai)\n",
    "- **sam** = three letter sample type (cal, raw, fil) for calibration, raw water (unfiltered) or filtered (fil)\n",
    "- **r** = numbered replicate (1, 2, 3) \n",
    "- **TXX_X** = temperature in Celcius (T17_3)\n",
    "    \n",
    "**Ex:** AC9_071618_y17_raw_a_1_T17_6.dat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Import the required python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import matplotlib.pylab as plt\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## from: https://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Inputs - should these be pre-defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Define directory where raw data is located\n",
    "raw_data_directory = \"0_renamed_originals\"\n",
    "### New directory name to store summary data\n",
    "new_dir_name = '1_summary_stats'\n",
    "### Directory name to stored cleaned raw data if only parts of the time series should be used\n",
    "cleaned_raw_data_name = '0.5_cleaned_originals'\n",
    "## Name for original metadata file which will be retained for posterity\n",
    "metadata_original_name = 'project_metadata_original.csv'\n",
    "## Name for copy of original metadata file which we will add to later\n",
    "metadata_updated_name = 'project_metadata_updated.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create new file paths and directories for referencing inputs and storing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Will be operating from Code directory - this gets parent directory path\n",
    "parent_directory = os.path.abspath('..')\n",
    "### Get path to raw data\n",
    "raw_data_file_path = parent_directory+'/Data/'+raw_data_directory\n",
    "### Get path to original metadata file that you will create for reference and not touch again\n",
    "metadata_original_filepath = parent_directory+'/Metadata/'+metadata_original_name\n",
    "### Get path to updatable metadata file that you will add to\n",
    "metadata_updated_filepath = parent_directory+'/Metadata/'+metadata_updated_name\n",
    "## Get full path name of new directory\n",
    "new_dir_path = os.path.abspath('..')+'/Data/'+new_dir_name\n",
    "cleaned_raw_data_path = os.path.abspath('..')+'/Data/'+cleaned_raw_data_name\n",
    "\n",
    "\n",
    "### Create metadata directory\n",
    "if not os.path.exists(parent_directory+'/Metadata'):\n",
    "    os.makedirs(parent_directory+'/Metadata')\n",
    "    \n",
    "### Create figures directory\n",
    "if not os.path.exists(parent_directory+'/Figures'):\n",
    "    os.makedirs(parent_directory+'/Figures')\n",
    "\n",
    "### Create directory for storing figures that show rolling average of standard deviations\n",
    "if not os.path.exists(parent_directory+'/Figures/Check_raw_stds'):\n",
    "    os.makedirs(parent_directory+'/Figures/Check_raw_stds')\n",
    "    \n",
    "## Get full path name of new directory\n",
    "new_dir_path = os.path.abspath('..')+'/Data/'+new_dir_name\n",
    "\n",
    "## Generate new directory if it doesn't already exist\n",
    "if not os.path.exists(new_dir_path):\n",
    "    os.makedirs(new_dir_path)\n",
    "    \n",
    "## Generate new directory if it doesn't already exist\n",
    "if not os.path.exists(cleaned_raw_data_path):\n",
    "    os.makedirs(cleaned_raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create metadata csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Metadata-original csv should contain only information that is pertinent to id'ing a sample; metadata-updated stores additional environmental information as well as file paths for various steps in the data-cleaning process, whether samples pass or fail quality checks, etc **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basenames = []\n",
    "date = []\n",
    "site = []\n",
    "sample_type = []\n",
    "abs_type = []\n",
    "rep = []\n",
    "temp = []\n",
    "raw_file_path = []\n",
    "\n",
    "\n",
    "file_list = sorted(glob.glob(raw_data_file_path+'/*.dat'))\n",
    "for j in file_list:\n",
    "    basename = j.split('/')[-1].split('.')[0]\n",
    "    ## We are excluding the field measurements of filtered data here because theyw ere not good \n",
    "    if 'field' in basename:\n",
    "        continue\n",
    "    basenames.append(basename)\n",
    "    Sensor, Date, Site, Sample_Type, Abs_Type, Rep, T1, T2= basename.split('_')\n",
    "    T = float(T1.lstrip('T')+'.'+T2)\n",
    "    date.append(str(Date))\n",
    "    site.append(Site)\n",
    "    sample_type.append(Sample_Type)\n",
    "    abs_type.append(Abs_Type)\n",
    "    rep.append(Rep)\n",
    "    temp.append(T)\n",
    "    raw_file_path.append(j)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'Sensor':'AC9',\n",
    "     'ID':basenames,\n",
    "     'Date': date,\n",
    "     'Site': site,\n",
    "     'Sample_Type': sample_type,\n",
    "     'Analysis_Type': abs_type,\n",
    "     'Rep': rep,\n",
    "    })\n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    {'Sensor':'AC9',\n",
    "     'ID':basenames,\n",
    "     'Date': date,\n",
    "     'Site': site,\n",
    "     'Sample_Type': sample_type,\n",
    "     'Analysis_Type': abs_type,\n",
    "     'Rep': rep,\n",
    "     'Temp': temp,\n",
    "     'Raw_File_Path':raw_file_path\n",
    "    })\n",
    "\n",
    "df.to_csv(metadata_original_filepath, sep='\\t',index=False)\n",
    "df2.to_csv(metadata_updated_filepath, sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate summary statistics and make new csv files for each sample\n",
    "\n",
    "#### Use the columns in the upper part of the document for the wavelengths. Just to organize your main dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:54: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    }
   ],
   "source": [
    "# iterate through the raw files\n",
    "metadata = pd.read_csv(metadata_updated_filepath, dtype={'Date': object}, skiprows=0, delimiter= '\\t')\n",
    "cleaned_raw_file_path = []\n",
    "averaged_file_path = []\n",
    "conf = []\n",
    "\n",
    "for file_path in metadata['Raw_File_Path']:\n",
    "# brings in file\n",
    "    ### Some field filtered measurements sneakily made it through; we shall eliminate them\n",
    "    if 'field' in file_path.split('/')[-1]:\n",
    "        continue\n",
    "    read_wl = pd.read_csv(file_path, skiprows=10, names=range(100), delimiter= '\\t')  # use names= range (100) to clip dangling columns\n",
    "    # reads in a and c wavelength values from the first column of data\n",
    "    a_c_wl = read_wl[0][0:18]  ; a_wl = read_wl[0][0:9] ; c_wl = read_wl[0][9:18];\n",
    "\n",
    "    # make empty objects for your new variables of the wavelength value and name\n",
    "    # Example: wl_a: 650.0; wl_a_str: a650\n",
    "    wl_a = []    ; wl_c = []  ; wl_a_str = []  ; wl_c_str = []\n",
    "\n",
    "    # makes a list of the 9 wavelengths formatted as floats\n",
    "    for i in range(len(a_wl)):\n",
    "        wl_a.append(np.float(a_wl[i][1:4]))\n",
    "        wl_a_str.append(a_wl[i])\n",
    "    for j in range(len(c_wl)):\n",
    "        wl_c.append(np.float(c_wl[j+9][1:4]))\n",
    "        wl_c_str.append(c_wl[j+9])\n",
    "\n",
    "     # Unsorted list of wavelengths (412) and wavelength strings (a676)\n",
    "    wavelist = wl_a + wl_c                   ; wavelist_str = wl_a_str + wl_c_str   \n",
    "    # Unsorted list of wavelengths (412) and wavelength strings (a676) as arrays\n",
    "    wavelengths = np.asarray(wavelist)       ; wavelengths_str = np.asarray(wavelist_str)  \n",
    "    # Sorted list of a and c wavelengths as floats in an array (ex: 412, 440, etc)\n",
    "    wl_a_sorted = np.asarray(sorted(wl_a))   ; wl_c_sorted = np.asarray(sorted(wl_c))\n",
    "\n",
    "    # Now read back in the data, skipping all the header information  \n",
    "    # The time series of measured values starts in the 32th row\n",
    "    df1 = pd.read_csv(file_path, skiprows=31, delimiter= '\\t') \n",
    "\n",
    "    # drops all the ragged extra columns dangling to the right\n",
    "    columns = df1.columns[19:len(df1.columns)]                 \n",
    "    df2 = df1.drop(columns, axis=1)    # you should have 19 cols left ~ array size [ntimesteps, 19]                       \n",
    "\n",
    "    # drops the first column of the timestamp (ntimesteps)\n",
    "    columns1 = df2.columns[0]\n",
    "    df3 = df2.drop(columns1, axis=1)                    \n",
    "\n",
    "    # makes a new header from the list of wavelengths you parsed earlier  \n",
    "    wl_header = wavelengths_str \n",
    "\n",
    "    # Clean and reindex\n",
    "    df4 = df3[1:]                                       # take the data (row 1- n) less the header row (row 0)\n",
    "    df4.columns = wavelengths_str                       # set the header row as list of wavelengths\n",
    "    df4 = df4.reindex_axis(sorted(df4.columns), axis=1) # reindex them by the new sorted wavelengths\n",
    "    df4=df4.convert_objects(convert_numeric=True)       # Just to make sure that all elements are floats!\n",
    "\n",
    "    no_cols = int(len(df4.columns)/2.)                  # no_col should always be 9 (one for each wavelength)         \n",
    "\n",
    "    # Sort your dataframe with ascending walues of your wavelengths\n",
    "    # at this point the wl_a and wl_c are the same wavelengths so \n",
    "    # it doesn't matter which one you use here\n",
    "    new_header = wl_a_sorted\n",
    "\n",
    "    # reindex to reshape the data\n",
    "    df_a_aux = df4.iloc[:, :no_cols];  df_a_aux.columns = new_header ;  df_a = df_a_aux.reindex_axis(sorted(df_a_aux.columns), axis = 1)\n",
    "    df_c_aux = df4.iloc[:, no_cols:];  df_c_aux.columns = new_header ;  df_c = df_c_aux.reindex_axis(sorted(df_c_aux.columns), axis = 1)\n",
    "\n",
    "    # calculate the me(di)an, stdev, IQR for the time series - per each column\n",
    "    a_mean = df_a[wl_a_sorted].mean(axis=0)         ; c_mean = df_c[wl_c_sorted].mean(axis=0)   \n",
    "    a_std  = df_a[wl_a_sorted].std(axis=0)          ; c_std  = df_c[wl_c_sorted].std(axis=0)\n",
    "    a_median = df_a[wl_a_sorted].median(axis=0)     ; c_median = df_c[wl_c_sorted].median(axis=0)\n",
    "    a_var = df_a[wl_a_sorted].var(axis=0)           ; c_var = df_c[wl_c_sorted].var(axis=0)\n",
    "    a_conf = mean_confidence_interval(df_a[wl_a_sorted]) ; c_conf = mean_confidence_interval(df_c[wl_c_sorted])\n",
    "\n",
    "    # Computing IQR\n",
    "    a_Q1 = df_a[wl_a_sorted].quantile(0.25)         ; c_Q1 = df_c[wl_c_sorted].quantile(0.25)\n",
    "    a_Q3 = df_a[wl_a_sorted].quantile(0.75)         ; c_Q3 = df_c[wl_c_sorted].quantile(0.75)\n",
    "    a_IQR = a_Q3 - a_Q1                             ; c_IQR = c_Q3 - c_Q1\n",
    "\n",
    "    # Specifiy the output file name and directory\n",
    "    metadata_row = metadata.loc[metadata['Raw_File_Path']==file_path]\n",
    "    ID = metadata_row['ID'].max()\n",
    "    sensor = metadata_row['Sensor'].max()\n",
    "    date = metadata_row['Date'].max()\n",
    "    site = metadata_row['Site'].max()\n",
    "    sample_type = metadata_row['Sample_Type'].max()\n",
    "    analysis_type = metadata_row['Analysis_Type'].max()\n",
    "    \n",
    "    # Removing temperature information from sample name because it is already recorded elsewhere\n",
    "    outputname = ID+ '.csv'\n",
    "    outputdir = new_dir_path+'/'+ outputname \n",
    "    cleaned_raw_outputdir = cleaned_raw_data_path+'/'+'cleaned_raw_'+outputname \n",
    "    cleaned_raw_file_path.append(cleaned_raw_outputdir)\n",
    "    averaged_file_path.append(outputdir)\n",
    "\n",
    "    # make a new dataframe from the summary statistics and export\n",
    "    if analysis_type == 'a':\n",
    "        conf.append(a_conf)\n",
    "        df_a.to_csv(cleaned_raw_outputdir, sep='\\t',index=False)\n",
    "        output_df = pd.DataFrame([wl_a_sorted, a_mean, a_std, a_median, a_var, a_IQR]).swapaxes(0,1)\n",
    "        output_df.columns = ('wl', 'a_mean', 'a_std', 'a_median', 'a_var','a_IQR')\n",
    "        output_df.to_csv(outputdir, sep='\\t',index=False)\n",
    "    else:\n",
    "        conf.append(c_conf)\n",
    "        df_c.to_csv(cleaned_raw_outputdir, sep='\\t',index=False)\n",
    "        output_df = pd.DataFrame([wl_c_sorted, c_mean, c_std, c_median, c_var, c_IQR]).swapaxes(0,1)\n",
    "        output_df.columns = ('wl', 'c_mean', 'c_std', 'c_median', 'c_var','c_IQR')\n",
    "        output_df.to_csv(outputdir, sep='\\t',index=False)\n",
    "\n",
    "metadata = pd.read_csv(metadata_updated_filepath, skiprows=0, dtype={'Date': object}, delimiter= '\\t')\n",
    "metadata['Cleaned_raw_file_path'] = cleaned_raw_file_path\n",
    "metadata['Summary_file_path'] = averaged_file_path\n",
    "metadata['Conf'] = [max(i) for i in conf]\n",
    "metadata.to_csv(metadata_updated_filepath, sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Graphing standard deviations over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Pick a size of window over which to calculate standard deviation\n",
    "window_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Read in metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_updated_filepath, skiprows=0, dtype={'Date': object}, delimiter= '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Function to generate standard deviations for cleaned raw data - takes rolling standard deviation using a user-specified window size for each wavelength, then averages the standard deviations to get a mean standard deviation. Returns a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_std_means(filepath_to_csv, window_size):\n",
    "    example = pd.read_csv(filepath_to_csv, skiprows=0, delimiter= '\\t')\n",
    "    #example_means = example.rolling(window=window_size).mean()[window_size-1:]\n",
    "    example_std = example.rolling(window=window_size).std()[window_size-1:]\n",
    "    example_std = example_std.reset_index()\n",
    "    example_std = example_std.rename(columns = {'index':'time'})\n",
    "    example_std['time'] = example_std['time'].apply(lambda x: (x+2)-window_size)\n",
    "    example_std['mean'] = example_std[example_std.columns[1:]].mean(axis=1)\n",
    "    return example_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Function to plot rolling average standard devitation, pooling the filtered, raw, and calibrated for each site with separate plots for attenuation and absorbance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plotting_rolling_avg(list_of_lists, window, title='Title'):\n",
    "    labels_list = []\n",
    "    for mylist in list_of_lists:           \n",
    "            repname = mylist[0]\n",
    "            labels_list.append(repname)\n",
    "            if 'fil' in repname:\n",
    "                linestyle = '--'\n",
    "            if 'raw' in repname:\n",
    "                linestyle = '-'\n",
    "            if 'cal' in repname:\n",
    "                linestyle = ':'\n",
    "            plt.plot('time','mean',data=mylist[1],label=repname,linestyle=linestyle)\n",
    "            #plt.scatter('wl', 'a_mean', data = df)\n",
    "    myxlabel = 'Start position for getting St. Dev. (end = start + %s)' % window\n",
    "    plt.xlabel(myxlabel)\n",
    "    plt.ylabel('a[1/m]')\n",
    "    plt.title(title)\n",
    "    plt.legend(labels_list)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Generates a dictionary using metadata information which puts samples into order for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_std_for_examination = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-12-0f11e656d113>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-0f11e656d113>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "metadata['SiteDate'] = metadata[\"Site\"] + '_' + metadata[\"Date\"].map(str)\n",
    "include_sitedate = []\n",
    "for index, row in metadata.iterrows():\n",
    "    Filepath = row['Cleaned_raw_file_path'] \n",
    "    std_dataframe = get_std_means(Filepath,window_size)\n",
    "    SiteDate = row['SiteDate']\n",
    "    if std_dataframe['mean'].max() > min_std_for_examination:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "metadata['SiteDate'] = metadata[\"Site\"] + '_' + metadata[\"Date\"].map(str)\n",
    "file_dict = {}\n",
    "for index, row in metadata.iterrows():\n",
    "    Filepath = row['Cleaned_raw_file_path'] \n",
    "    std_dataframe = get_std_means(Filepath,window_size)\n",
    "    if std_dataframe['mean'].max() > min_std_for_examination:\n",
    "        SiteDate = row['SiteDate']\n",
    "        Analysis_Type = row['Analysis_Type']\n",
    "        repname = row['Sample_Type']+' '+str(row['Rep'])\n",
    "        if SiteDate not in file_dict:\n",
    "            file_dict[SiteDate] = {'a':[], 'c':[]}\n",
    "        if Analysis_Type == 'a':\n",
    "            file_dict[SiteDate]['a'].append([repname,std_dataframe])\n",
    "        if Analysis_Type == 'c':\n",
    "            file_dict[SiteDate]['c'].append([repname,std_dataframe])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# NOTE for problem that needs to be fixed: using the 'subplots' command, when I plot for the first time on the kernel a blank plot is the first to appear. Not sure why. Ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Making pretty plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_types = {'fil':\"Filtered\",'raw':'Unfiltered (Raw)','cal':\"Calibration\"}\n",
    "abs_types = {'a':'Absorption','c':'Attenuation'}\n",
    "types_list = ['a','c']\n",
    "\n",
    "for i in file_dict:\n",
    "    Site_all, Date_all = i.split('_')\n",
    "    plt.subplots(1, 2, figsize=(12, 5.5))\n",
    "    count = 0\n",
    "    for j in types_list:\n",
    "        count += 1\n",
    "        plt.subplot(1, 2, count)    \n",
    "        my_plot = plotting_rolling_avg(file_dict[i][j], title=abs_types[j], window = window_size)\n",
    "        #dict_of_sampleids_and_plots[sampleid] = my_plot\n",
    "    plt.subplots_adjust(top=0.88, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                    wspace=0.35)\n",
    "    my_plot.suptitle('Site: '+Site_all+' '+'Date: '+Date_all, fontsize=20)\n",
    "    my_plot.savefig('../Figures/Check_raw_stds/'+Site_all+'_'+Date_all+'.png', bbox_inches='tight')\n",
    "    my_plot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
