{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ABOVE | AC9 Data Processing\n",
    "***\n",
    "## 01 Read AC9 files\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Authors:** Catherine Kuhn and Elena TerziÄ‡ and Anna Simpson\n",
    "**Last Updated:** August, 29th, 2018\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code reads in raw ac9 .dat files and parses sample information from the filename and header information inside the file. The output is a table of summary statistics formatted as a *.csv* file for each wavelength for each file. This code was built for a worflow in which a and c sides are sampled separately. File names should contain: date, site, rep, a or c side and water temperature. \n",
    "\n",
    "File names are formatted like ** AC9_dddddd_sit_sam_s_r_TXX_XX.dat** where:\n",
    "\n",
    "- **dddddd** = date (071718)\n",
    "- **sit** = three letter site code (fai)\n",
    "- **sam** = three letter sample type (cal, raw, fil) for calibration, raw water (unfiltered) or filtered (fil)\n",
    "- **r** = numbered replicate (1, 2, 3) \n",
    "- **TXX_X** = temperature in Celcius (T17_3)\n",
    "    \n",
    "**Ex:** AC9_071618_y17_raw_a_1_T17_6.dat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Import the required python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define directory where raw data is located\n",
    "raw_data_directory = \"1_1_renamed_originals\"\n",
    "### New directory name to store summary data\n",
    "new_dir_name = '2_summary_stats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parent_directory = os.path.abspath('..')\n",
    "raw_data_file_path = parent_directory+'/Data/'+raw_data_directory\n",
    "# Get a list of all the subdirectories in the directory\n",
    "directories = os.listdir(raw_data_file_path)\n",
    "# This removes any hidden files that os.listdir is picking up from your list of subdirectories\n",
    "# (was picking up .DS_store)\n",
    "directories = [subdirectory for subdirectory in directories if not subdirectory.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary  storing subdirectory/site names (keys) \n",
    "# and lists of files within each subdirectory (entries)\n",
    "site_files_dict = {}\n",
    "# Iterate through subdirectories\n",
    "for i in directories:\n",
    "    path=raw_data_file_path+'/'+i+'/'\n",
    "    # Get a list of the files in each subdirectory as the entry for each site name key\n",
    "    site_files_dict[i] = sorted(glob.glob(path+'*.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define the new directory where summary stats will be stored\n",
    "\n",
    "#### Default action: summary stat files will be stored in subdirectories (defined by site names) in the new directory you have defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Get full path name of new directory\n",
    "new_dir_path = os.path.abspath('..')+'/Data/'+new_dir_name\n",
    "\n",
    "## Generate new directory if it doesn't already exist\n",
    "if not os.path.exists(new_dir_path):\n",
    "    os.makedirs(new_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate summary statistics and make new csv files for each sample\n",
    "\n",
    "#### Use the columns in the upper part of the document for the wavelengths. Just to organize your main dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7d2b349c59a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# parses temperature, samplename, a or c side and site from the file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# 0 = A, 0:3 takes AC9 (letters 0, 1, 2 not 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtemp_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msample_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0ma_or_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m         \u001b[0msite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m   \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: "
     ]
    }
   ],
   "source": [
    "# iterate through dictionary key entries (site names)\n",
    "for site_name in site_files_dict:\n",
    "    # define the path for each new subdirectory that will be made based on site name\n",
    "    individual_site_directory_path = new_dir_path+'/'+site_name\n",
    "    # generate the new subdirectory for each site if it doesn't already exist\n",
    "    if not os.path.exists(individual_site_directory_path):\n",
    "        os.makedirs(individual_site_directory_path)\n",
    "    # Get the list of original raw files for this site\n",
    "    site_files = site_files_dict[site_name]\n",
    "    # Iterate through that list of files so that:\n",
    "    # each file can be imported ad each summary stat file can be generated\n",
    "    for file in range(len(site_files)):\n",
    "    # brings in file\n",
    "        filename = site_files[file].split('/')[-1]\n",
    "        read_wl = pd.read_csv(site_files[file], skiprows=10, names=range(100), delimiter= '\\t')  # use names= range (100) to clip dangling columns\n",
    "        # reads in a and c wavelength values from the first column of data\n",
    "        a_c_wl = read_wl[0][0:18]  ; a_wl = read_wl[0][0:9] ; c_wl = read_wl[0][9:18];\n",
    "\n",
    "        # parses temperature, samplename, a or c side and site from the file name\n",
    "        # 0 = A, 0:3 takes AC9 (letters 0, 1, 2 not 3)\n",
    "        temperature = float(filename[24:28].replace('_', '.'))\n",
    "        temp_string = filename[24:28]; sample_type = filename[15:18];  a_or_c = filename[19:20];   \n",
    "        date = filename[4:10];         site = filename[11:14];   rep = filename[21:22];\n",
    "\n",
    "        # make empty objects for your new variables of the wavelength value and name\n",
    "        # Example: wl_a: 650.0; wl_a_str: a650\n",
    "        wl_a = []    ; wl_c = []  ; wl_a_str = []  ; wl_c_str = []\n",
    "\n",
    "        # makes a list of the 9 wavelengths formatted as floats\n",
    "        for i in range(len(a_wl)):\n",
    "            wl_a.append(np.float(a_wl[i][1:4]))\n",
    "            wl_a_str.append(a_wl[i])\n",
    "        for j in range(len(c_wl)):\n",
    "            wl_c.append(np.float(c_wl[j+9][1:4]))\n",
    "            wl_c_str.append(c_wl[j+9])\n",
    "\n",
    "         # Unsorted list of wavelengths (412) and wavelength strings (a676)\n",
    "        wavelist = wl_a + wl_c                   ; wavelist_str = wl_a_str + wl_c_str   \n",
    "        # Unsorted list of wavelengths (412) and wavelength strings (a676) as arrays\n",
    "        wavelengths = np.asarray(wavelist)       ; wavelengths_str = np.asarray(wavelist_str)  \n",
    "        # Sorted list of a and c wavelengths as floats in an array (ex: 412, 440, etc)\n",
    "        wl_a_sorted = np.asarray(sorted(wl_a))   ; wl_c_sorted = np.asarray(sorted(wl_c))\n",
    "\n",
    "        # Now read back in the data, skipping all the header information  \n",
    "        # The time series of measured values starts in the 32th row\n",
    "        df1 = pd.read_csv(site_files[file], skiprows=31, delimiter= '\\t') \n",
    "\n",
    "        # drops all the ragged extra columns dangling to the right\n",
    "        columns = df1.columns[19:len(df1.columns)]                 \n",
    "        df2 = df1.drop(columns, axis=1)    # you should have 19 cols left ~ array size [ntimesteps, 19]                       \n",
    "\n",
    "        # drops the first column of the timestamp (ntimesteps)\n",
    "        columns1 = df2.columns[0]\n",
    "        df3 = df2.drop(columns1, axis=1)                    \n",
    "\n",
    "        # makes a new header from the list of wavelengths you parsed earlier  \n",
    "        wl_header = wavelengths_str \n",
    "\n",
    "        # Clean and reindex\n",
    "        df4 = df3[1:]                                       # take the data (row 1- n) less the header row (row 0)\n",
    "        df4.columns = wavelengths_str                       # set the header row as list of wavelengths\n",
    "        df4 = df4.reindex_axis(sorted(df4.columns), axis=1) # reindex them by the new sorted wavelengths\n",
    "        df4=df4.convert_objects(convert_numeric=True)       # Just to make sure that all elements are floats!\n",
    "\n",
    "        no_cols = int(len(df4.columns)/2.)                  # no_col should always be 9 (one for each wavelength)         \n",
    "\n",
    "        # Sort your dataframe with ascending walues of your wavelengths\n",
    "        # at this point the wl_a and wl_c are the same wavelengths so \n",
    "        # it doesn't matter which one you use here\n",
    "        new_header = wl_a_sorted\n",
    "\n",
    "        # reindex to reshape the data\n",
    "        df_a_aux = df4.iloc[:, :no_cols];  df_a_aux.columns = new_header ;  df_a = df_a_aux.reindex_axis(sorted(df_a_aux.columns), axis = 1)\n",
    "        df_c_aux = df4.iloc[:, no_cols:];  df_c_aux.columns = new_header ;  df_c = df_c_aux.reindex_axis(sorted(df_c_aux.columns), axis = 1)\n",
    "\n",
    "        # calculate the me(di)an, stdev, IQR for the time series - per each column\n",
    "        a_mean = df_a[wl_a_sorted].mean(axis=0)         ; c_mean = df_c[wl_c_sorted].mean(axis=0)   \n",
    "        a_std  = df_a[wl_a_sorted].std(axis=0)          ; c_std  = df_c[wl_c_sorted].std(axis=0)\n",
    "        a_median = df_a[wl_a_sorted].median(axis=0)     ; c_median = df_c[wl_c_sorted].median(axis=0)\n",
    "\n",
    "        # Computing IQR\n",
    "        a_Q1 = df_a[wl_a_sorted].quantile(0.25)         ; c_Q1 = df_c[wl_c_sorted].quantile(0.25)\n",
    "        a_Q3 = df_a[wl_a_sorted].quantile(0.75)         ; c_Q3 = df_c[wl_c_sorted].quantile(0.75)\n",
    "        a_IQR = a_Q3 - a_Q1                             ; c_IQR = c_Q3 - c_Q1\n",
    "\n",
    "        # Specifiy the output file name and directory\n",
    "        outputname = 'AC9_' + str(date) + '_'+ str(site) +'_' + str(sample_type) + '_' + str(a_or_c) +'_' + str(rep)+ '_'  + 'T' + str(temp_string) + '.csv'\n",
    "        outputdir = str(parent_directory)+'/'+str(new_dir_name)+'/'+site_name+'/'+ outputname \n",
    "\n",
    "        # make a new dataframe from the summary statistics and export\n",
    "        if a_or_c == 'a':\n",
    "            output_df = pd.DataFrame([wl_a_sorted, a_mean, a_std, a_median, a_IQR]).swapaxes(0,1)\n",
    "            output_df.columns = ('wl', 'a_mean', 'a_std', 'a_median', 'a_IQR')\n",
    "            output_df.to_csv(outputdir, sep='\\t')\n",
    "        else:\n",
    "            output_df = pd.DataFrame([wl_c_sorted, c_mean, c_std, c_median, c_IQR]).swapaxes(0,1)\n",
    "            output_df.columns = ('wl', 'c_mean', 'c_std', 'c_median', 'c_IQR')\n",
    "            output_df.to_csv(outputdir, sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
