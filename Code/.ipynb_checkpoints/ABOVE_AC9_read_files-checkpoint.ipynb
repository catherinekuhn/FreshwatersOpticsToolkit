{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ABOVE | AC9 Data Processing\n",
    "***\n",
    "## Step 01 Read AC9 files\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Authors:** Catherine Kuhn and Elena TerziÄ‡ and Anna Simpson\n",
    "**Last Updated:** August, 29th, 2018\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code reads in raw ac9 .dat files and parses sample information from the filename and header information inside the file. The output is a table of summary statistics formatted as a *.csv* file for each wavelength for each file. This code was built for a worflow in which a and c sides are sampled separately. File names should contain: date, site, rep, a or c side and water temperature. \n",
    "\n",
    "File names are formatted like ** AC9_dddddd_sit_sam_s_r_TXX_XX.dat** where:\n",
    "\n",
    "- **dddddd** = date (071718)\n",
    "- **sit** = three letter site code (fai)\n",
    "- **sam** = three letter sample type (cal, raw, fil) for calibration, raw water (unfiltered) or filtered (fil)\n",
    "- **r** = numbered replicate (1, 2, 3) \n",
    "- **TXX_X** = temperature in Celcius (T17_3)\n",
    "    \n",
    "**Ex:** AC9_071618_y17_raw_a_1_T17_6.dat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Import the required python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Define directory where raw data is located\n",
    "raw_data_directory = \"1_1_renamed_originals\"\n",
    "### New directory name to store summary data\n",
    "new_dir_name = '2_summary_stats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Will be operating from Code directory - this gets parent directory path\n",
    "parent_directory = os.path.abspath('..')\n",
    "### Get path to raw data\n",
    "raw_data_file_path = parent_directory+'/Data/'+raw_data_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create metadata csv\n",
    "\n",
    "raw_data_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basenames = []\n",
    "date = []\n",
    "site = []\n",
    "sample_type = []\n",
    "abs_type = []\n",
    "rep = []\n",
    "temp = []\n",
    "\n",
    "\n",
    "file_list = sorted(glob.glob(raw_data_file_path+'/*.dat'))\n",
    "for j in file_list:\n",
    "    basename = j.split('/')[-1].split('.')[0]\n",
    "    ## We are excluding the field measurements of filtered data here because theyw ere not good \n",
    "    if 'field' in basename:\n",
    "        continue\n",
    "    basenames.append(basename)\n",
    "    Sensor, Date, Site, Sample_Type, Abs_Type, Rep, T1, T2= basename.split('_')\n",
    "    T = float(T1.lstrip('T')+'.'+T2)\n",
    "    date.append(Date)\n",
    "    site.append(Site)\n",
    "    sample_type.append(Sample_Type)\n",
    "    abs_type.append(Abs_Type)\n",
    "    rep.append(Rep)\n",
    "    temp.append(T)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'Sensor':'AC9',\n",
    "     'ID':basenames,\n",
    "     'Date': date,\n",
    "     'Site': site,\n",
    "     'Sample_Type': sample_type,\n",
    "     'Analysis_Type': abs_type,\n",
    "     'Rep': rep,\n",
    "     'Temp': temp,\n",
    "    })\n",
    "if not os.path.exists(parent_directory+'/Metadata'):\n",
    "    os.makedirs(parent_directory+'/Metadata')\n",
    "df.to_csv(parent_directory+'/Metadata/project_metadata_original.csv', sep='\\t')\n",
    "df.to_csv(parent_directory+'/Metadata/project_metadata_updated.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add raw file paths to metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define the new directory where summary stats will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Get full path name of new directory\n",
    "new_dir_path = os.path.abspath('..')+'/Data/'+new_dir_name\n",
    "\n",
    "## Generate new directory if it doesn't already exist\n",
    "if not os.path.exists(new_dir_path):\n",
    "    os.makedirs(new_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate summary statistics and make new csv files for each sample\n",
    "\n",
    "#### Use the columns in the upper part of the document for the wavelengths. Just to organize your main dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AC9_070718_sea_cal_a_1_T20_6.dat\n",
      "AC9_070718_sea_cal_a_2_T20_9.dat\n",
      "AC9_070718_sea_cal_a_3_T20_9.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:58: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AC9_070718_sea_cal_c_1_T20_6.dat\n",
      "AC9_070718_sea_cal_c_2_T20_6.dat\n",
      "AC9_070718_sea_cal_c_3_T20_6.dat\n",
      "AC9_071318_bai_cal_a_1_T23_8.dat\n",
      "AC9_071318_bai_cal_a_2_T24_1.dat\n",
      "AC9_071318_bai_cal_a_3_T24_0.dat\n",
      "AC9_071318_bai_cal_c_1_T23_7.dat\n",
      "AC9_071318_bai_cal_c_2_T23_8.dat\n",
      "AC9_071318_bai_cal_c_3_T23_9.dat\n",
      "AC9_071318_fai_cal_a_1_T23_8.dat\n",
      "AC9_071318_fai_cal_a_2_T24_1.dat\n",
      "AC9_071318_fai_cal_a_3_T24_0.dat\n",
      "AC9_071318_fai_cal_c_1_T23_7.dat\n",
      "AC9_071318_fai_cal_c_2_T23_8.dat\n",
      "AC9_071318_fai_cal_c_3_T23_9.dat\n",
      "AC9_071418_bai_fil_a_1_T13_4.dat\n",
      "AC9_071418_bai_fil_a_2_T19_2.dat\n",
      "AC9_071418_bai_fil_a_3_T18_5.dat\n",
      "AC9_071418_bai_fil_c_1_T13_4.dat\n",
      "AC9_071418_bai_fil_c_2_T19_2.dat\n",
      "AC9_071418_bai_fil_c_3_T18_5.dat\n",
      "AC9_071418_bai_raw_a_1_T19_8.dat\n",
      "AC9_071418_bai_raw_a_2_T19_8.dat\n",
      "AC9_071418_bai_raw_a_3_T17_8.dat\n",
      "AC9_071418_bai_raw_c_1_T19_8.dat\n",
      "AC9_071418_bai_raw_c_2_T19_8.dat\n",
      "AC9_071418_bai_raw_c_3_T17_8.dat\n",
      "AC9_071618_can_cal_a_1_T19_3.dat\n",
      "AC9_071618_can_cal_a_2_T19_9.dat\n",
      "AC9_071618_can_cal_a_3_T19_0.dat\n",
      "AC9_071618_can_cal_c_1_T19_3.dat\n",
      "AC9_071618_can_cal_c_2_T19_9.dat\n",
      "AC9_071618_can_cal_c_3_T19_0.dat\n",
      "AC9_071618_cbr_cal_a_1_T19_9.dat\n",
      "AC9_071618_cbr_cal_a_2_T19_3.dat\n",
      "AC9_071618_cbr_cal_a_3_T19_0.dat\n",
      "AC9_071618_cbr_cal_c_1_T19_9.dat\n",
      "AC9_071618_cbr_cal_c_2_T19_3.dat\n",
      "AC9_071618_cbr_cal_c_3_T19_0.dat\n",
      "AC9_071618_cbr_fil_a_1_T17_5.dat\n",
      "AC9_071618_cbr_fil_a_2_T17_5.dat\n",
      "AC9_071618_cbr_fil_a_3_T17_5.dat\n",
      "AC9_071618_cbr_fil_c_1_T17_5.dat\n",
      "AC9_071618_cbr_fil_c_2_T17_5.dat\n",
      "AC9_071618_cbr_fil_c_3_T17_5.dat\n",
      "AC9_071618_cbr_raw_a_1_T17_5.dat\n",
      "AC9_071618_cbr_raw_a_2_T17_5.dat\n",
      "AC9_071618_cbr_raw_a_3_T17_5.dat\n",
      "AC9_071618_cbr_raw_c_1_T17_5.dat\n",
      "AC9_071618_cbr_raw_c_2_T17_5.dat\n",
      "AC9_071618_cbr_raw_c_3_T17_5.dat\n",
      "AC9_071718_9mi_cal_a_1_T19_0.dat\n",
      "AC9_071718_9mi_cal_c_1_T19_0.dat\n",
      "AC9_071718_9mi_fil_a_1_T15_4.dat\n",
      "AC9_071718_9mi_fil_a_2_T15_4.dat\n",
      "AC9_071718_9mi_fil_a_3_T15_1.dat\n",
      "AC9_071718_9mi_fil_c_1_T15_4.dat\n",
      "AC9_071718_9mi_fil_c_2_T15_4.dat\n",
      "AC9_071718_9mi_fil_c_3_T15_1.dat\n",
      "AC9_071718_9mi_raw_a_1_T17_4.dat\n",
      "AC9_071718_9mi_raw_a_2_T17_4.dat\n",
      "AC9_071718_9mi_raw_a_3_T17_4.dat\n",
      "AC9_071718_9mi_raw_c_1_T17_4.dat\n",
      "AC9_071718_9mi_raw_c_2_T17_4.dat\n",
      "AC9_071718_9mi_raw_c_3_T17_4.dat\n",
      "AC9_071718_can_cal_a_1_T19_0.dat\n",
      "AC9_071718_can_cal_a_2_T19_0.dat\n",
      "AC9_071718_can_cal_a_3_T19_0.dat\n",
      "AC9_071718_can_cal_c_1_T19_0.dat\n",
      "AC9_071718_can_cal_c_2_T19_0.dat\n",
      "AC9_071718_can_cal_c_3_T19_0.dat\n",
      "AC9_071718_sco_fil_a_2_T17_7.dat\n",
      "AC9_071718_sco_fil_a_3_T17_7.dat\n",
      "AC9_071718_sco_fil_c_2_T17_7.dat\n",
      "AC9_071718_sco_fil_c_3_T17_7.dat\n",
      "AC9_071718_sco_raw_a_1_T18_3.dat\n",
      "AC9_071718_sco_raw_a_2_T18_3.dat\n",
      "AC9_071718_sco_raw_a_3_T18_3.dat\n",
      "AC9_071718_sco_raw_c_1_T18_3.dat\n",
      "AC9_071718_sco_raw_c_2_T18_3.dat\n",
      "AC9_071718_sco_raw_c_3_T18_3.dat\n",
      "AC9_071718_y17_fil_a_1_T17_4.dat\n",
      "AC9_071718_y17_fil_a_2_T17_4.dat\n",
      "AC9_071718_y17_fil_c_1_T17_4.dat\n",
      "AC9_071718_y17_fil_c_2_T17_4.dat\n",
      "AC9_071718_y17_raw_a_2_T17_1.dat\n",
      "AC9_071718_y17_raw_a_3_T17_1.dat\n",
      "AC9_071718_y17_raw_a_4_T17_1.dat\n",
      "AC9_071718_y17_raw_c_2_T17_1.dat\n",
      "AC9_071718_y17_raw_c_3_T17_1.dat\n",
      "AC9_071718_y17_raw_c_4_T17_1.dat\n",
      "AC9_071718_y20_fil_a_1_T17_0.dat\n",
      "AC9_071718_y20_fil_a_2_T19_7.dat\n",
      "AC9_071718_y20_fil_c_1_T17_0.dat\n",
      "AC9_071718_y20_fil_c_2_T19_7.dat\n",
      "AC9_071718_y20_raw_a_1_T16_8.dat\n",
      "AC9_071718_y20_raw_a_2_T16_8.dat\n",
      "AC9_071718_y20_raw_a_3_T16_8.dat\n",
      "AC9_071718_y20_raw_c_1_T16_8.dat\n",
      "AC9_071718_y20_raw_c_2_T16_8.dat\n",
      "AC9_071718_y20_raw_c_3_T16_8.dat\n",
      "AC9_071818_boo_fil_a_1_T22_3.dat\n",
      "AC9_071818_boo_fil_a_2_T22_3.dat\n",
      "AC9_071818_boo_fil_a_3_T21_9.dat\n",
      "AC9_071818_boo_fil_c_1_T22_3.dat\n",
      "AC9_071818_boo_fil_c_2_T22_3.dat\n",
      "AC9_071818_boo_fil_c_3_T21_9.dat\n",
      "AC9_071818_boo_raw_a_1_T17_0.dat\n",
      "AC9_071818_boo_raw_a_2_T17_0.dat\n",
      "AC9_071818_boo_raw_a_3_T17_0.dat\n",
      "AC9_071818_boo_raw_c_1_T17_0.dat\n",
      "AC9_071818_boo_raw_c_2_T17_0.dat\n",
      "AC9_071818_boo_raw_c_3_T17_0.dat\n",
      "AC9_071818_can_cal_a_1_T21_2.dat\n",
      "AC9_071818_can_cal_a_2_T21_2.dat\n",
      "AC9_071818_can_cal_a_3_T21_2.dat\n",
      "AC9_071818_can_cal_c_1_T21_2.dat\n",
      "AC9_071818_can_cal_c_2_T21_2.dat\n",
      "AC9_071818_can_cal_c_3_T21_2.dat\n",
      "AC9_071818_gre_fil_a_1_T17_1_field.dat\n",
      "AC9_071818_gre_fil_a_1_T22_4.dat\n",
      "AC9_071818_gre_fil_a_2_T22_3.dat\n",
      "AC9_071818_gre_fil_a_3_T22_0.dat\n",
      "AC9_071818_gre_fil_c_1_T17_1_field.dat\n",
      "AC9_071818_gre_fil_c_1_T22_4.dat\n",
      "AC9_071818_gre_fil_c_2_T22_3.dat\n",
      "AC9_071818_gre_fil_c_3_T22_0.dat\n",
      "AC9_071818_gre_raw_a_1_T17_1.dat\n",
      "AC9_071818_gre_raw_a_2_T17_1.dat\n",
      "AC9_071818_gre_raw_a_3_T17_1.dat\n",
      "AC9_071818_gre_raw_c_1_T17_1.dat\n",
      "AC9_071818_gre_raw_c_2_T17_1.dat\n",
      "AC9_071818_gre_raw_c_3_T17_1.dat\n",
      "AC9_071918_can_cal_a_1_T21_6.dat\n",
      "AC9_071918_can_cal_a_2_T20_4.dat\n",
      "AC9_071918_can_cal_a_3_T20_1.dat\n",
      "AC9_071918_can_cal_c_1_T21_6.dat\n",
      "AC9_071918_can_cal_c_2_T20_4.dat\n",
      "AC9_071918_can_cal_c_3_T20_1.dat\n",
      "AC9_071918_cbr_fil_a_1_T19_8.dat\n",
      "AC9_071918_cbr_fil_a_2_T19_8.dat\n",
      "AC9_071918_cbr_fil_a_3_T19_2.dat\n",
      "AC9_071918_cbr_fil_c_1_T19_8.dat\n",
      "AC9_071918_cbr_fil_c_2_T19_8.dat\n",
      "AC9_071918_cbr_fil_c_3_T19_2.dat\n",
      "AC9_071918_cbr_raw_a_2_T16_1.dat\n",
      "AC9_071918_cbr_raw_a_3_T16_1.dat\n",
      "AC9_071918_cbr_raw_a_4_T16_1.dat\n",
      "AC9_071918_cbr_raw_c_2_T16_1.dat\n",
      "AC9_071918_cbr_raw_c_3_T16_1.dat\n",
      "AC9_071918_cbr_raw_c_4_T16_1.dat\n",
      "AC9_071918_y17_fil_a_1_T18_3.dat\n",
      "AC9_071918_y17_fil_a_2_T18_0.dat\n",
      "AC9_071918_y17_fil_a_3_T17_9.dat\n",
      "AC9_071918_y17_fil_c_1_T18_3.dat\n",
      "AC9_071918_y17_fil_c_2_T18_0.dat\n",
      "AC9_071918_y17_fil_c_3_T17_9.dat\n",
      "AC9_071918_y20_fil_a_1_T17_7.dat\n",
      "AC9_071918_y20_fil_a_2_T17_8.dat\n",
      "AC9_071918_y20_fil_a_3_T17_6.dat\n",
      "AC9_071918_y20_fil_a_4_T17_5.dat\n",
      "AC9_071918_y20_fil_c_1_T17_7.dat\n",
      "AC9_071918_y20_fil_c_2_T17_8.dat\n",
      "AC9_071918_y20_fil_c_3_T17_6.dat\n",
      "AC9_071918_y20_fil_c_4_T17_5.dat\n",
      "AC9_072118_fai_cal_a_1_T26_7.dat\n",
      "AC9_072118_fai_cal_a_2_T25_7.dat\n",
      "AC9_072118_fai_cal_a_3_T25_4.dat\n",
      "AC9_072118_fai_cal_c_1_T26_7.dat\n",
      "AC9_072118_fai_cal_c_2_T25_7.dat\n",
      "AC9_072118_fai_cal_c_3_T25_4.dat\n",
      "AC9_072118_fav_cal_a_1_T26_3.dat\n",
      "AC9_072118_fav_cal_c_1_T26_3.dat\n",
      "AC9_081618_sea_cal_a_1_T18_5.dat\n",
      "AC9_081618_sea_cal_a_2_T18_5.dat\n",
      "AC9_081618_sea_cal_a_3_T18_5.dat\n",
      "AC9_081618_sea_cal_a_4_T18_5.dat\n",
      "AC9_081618_sea_cal_c_1_T18_5.dat\n",
      "AC9_081618_sea_cal_c_2_T18_5.dat\n",
      "AC9_081618_sea_cal_c_3_T18_5.dat\n",
      "AC9_081618_sea_cal_c_4_T18_5.dat\n"
     ]
    }
   ],
   "source": [
    "# iterate through the raw files\n",
    "site_files = sorted(glob.glob(raw_data_file_path+'/*.dat'))\n",
    "for myfile in range(len(site_files)):\n",
    "# brings in file\n",
    "    filename = site_files[myfile].split('/')[-1]\n",
    "    ### Some field filtered measurements sneakily made it through; we shall eliminate them\n",
    "    if 'field' in filename:\n",
    "        continue\n",
    "    print filename\n",
    "    read_wl = pd.read_csv(site_files[myfile], skiprows=10, names=range(100), delimiter= '\\t')  # use names= range (100) to clip dangling columns\n",
    "    # reads in a and c wavelength values from the first column of data\n",
    "    a_c_wl = read_wl[0][0:18]  ; a_wl = read_wl[0][0:9] ; c_wl = read_wl[0][9:18];\n",
    "\n",
    "    # parses temperature, samplename, a or c side and site from the file name\n",
    "    # 0 = A, 0:3 takes AC9 (letters 0, 1, 2 not 3)\n",
    "    temperature = float(filename[24:28].replace('_', '.'))\n",
    "    temp_string = filename[24:28]; sample_type = filename[15:18];  a_or_c = filename[19:20];   \n",
    "    date = filename[4:10];         site = filename[11:14];   rep = filename[21:22];\n",
    "\n",
    "    # make empty objects for your new variables of the wavelength value and name\n",
    "    # Example: wl_a: 650.0; wl_a_str: a650\n",
    "    wl_a = []    ; wl_c = []  ; wl_a_str = []  ; wl_c_str = []\n",
    "\n",
    "    # makes a list of the 9 wavelengths formatted as floats\n",
    "    for i in range(len(a_wl)):\n",
    "        wl_a.append(np.float(a_wl[i][1:4]))\n",
    "        wl_a_str.append(a_wl[i])\n",
    "    for j in range(len(c_wl)):\n",
    "        wl_c.append(np.float(c_wl[j+9][1:4]))\n",
    "        wl_c_str.append(c_wl[j+9])\n",
    "\n",
    "     # Unsorted list of wavelengths (412) and wavelength strings (a676)\n",
    "    wavelist = wl_a + wl_c                   ; wavelist_str = wl_a_str + wl_c_str   \n",
    "    # Unsorted list of wavelengths (412) and wavelength strings (a676) as arrays\n",
    "    wavelengths = np.asarray(wavelist)       ; wavelengths_str = np.asarray(wavelist_str)  \n",
    "    # Sorted list of a and c wavelengths as floats in an array (ex: 412, 440, etc)\n",
    "    wl_a_sorted = np.asarray(sorted(wl_a))   ; wl_c_sorted = np.asarray(sorted(wl_c))\n",
    "\n",
    "    # Now read back in the data, skipping all the header information  \n",
    "    # The time series of measured values starts in the 32th row\n",
    "    df1 = pd.read_csv(site_files[myfile], skiprows=31, delimiter= '\\t') \n",
    "\n",
    "    # drops all the ragged extra columns dangling to the right\n",
    "    columns = df1.columns[19:len(df1.columns)]                 \n",
    "    df2 = df1.drop(columns, axis=1)    # you should have 19 cols left ~ array size [ntimesteps, 19]                       \n",
    "\n",
    "    # drops the first column of the timestamp (ntimesteps)\n",
    "    columns1 = df2.columns[0]\n",
    "    df3 = df2.drop(columns1, axis=1)                    \n",
    "\n",
    "    # makes a new header from the list of wavelengths you parsed earlier  \n",
    "    wl_header = wavelengths_str \n",
    "\n",
    "    # Clean and reindex\n",
    "    df4 = df3[1:]                                       # take the data (row 1- n) less the header row (row 0)\n",
    "    df4.columns = wavelengths_str                       # set the header row as list of wavelengths\n",
    "    df4 = df4.reindex_axis(sorted(df4.columns), axis=1) # reindex them by the new sorted wavelengths\n",
    "    df4=df4.convert_objects(convert_numeric=True)       # Just to make sure that all elements are floats!\n",
    "\n",
    "    no_cols = int(len(df4.columns)/2.)                  # no_col should always be 9 (one for each wavelength)         \n",
    "\n",
    "    # Sort your dataframe with ascending walues of your wavelengths\n",
    "    # at this point the wl_a and wl_c are the same wavelengths so \n",
    "    # it doesn't matter which one you use here\n",
    "    new_header = wl_a_sorted\n",
    "\n",
    "    # reindex to reshape the data\n",
    "    df_a_aux = df4.iloc[:, :no_cols];  df_a_aux.columns = new_header ;  df_a = df_a_aux.reindex_axis(sorted(df_a_aux.columns), axis = 1)\n",
    "    df_c_aux = df4.iloc[:, no_cols:];  df_c_aux.columns = new_header ;  df_c = df_c_aux.reindex_axis(sorted(df_c_aux.columns), axis = 1)\n",
    "\n",
    "    # calculate the me(di)an, stdev, IQR for the time series - per each column\n",
    "    a_mean = df_a[wl_a_sorted].mean(axis=0)         ; c_mean = df_c[wl_c_sorted].mean(axis=0)   \n",
    "    a_std  = df_a[wl_a_sorted].std(axis=0)          ; c_std  = df_c[wl_c_sorted].std(axis=0)\n",
    "    a_median = df_a[wl_a_sorted].median(axis=0)     ; c_median = df_c[wl_c_sorted].median(axis=0)\n",
    "    a_var = df_a[wl_a_sorted].var(axis=0)           ; c_var = df_c[wl_c_sorted].var(axis=0)\n",
    "\n",
    "    # Computing IQR\n",
    "    a_Q1 = df_a[wl_a_sorted].quantile(0.25)         ; c_Q1 = df_c[wl_c_sorted].quantile(0.25)\n",
    "    a_Q3 = df_a[wl_a_sorted].quantile(0.75)         ; c_Q3 = df_c[wl_c_sorted].quantile(0.75)\n",
    "    a_IQR = a_Q3 - a_Q1                             ; c_IQR = c_Q3 - c_Q1\n",
    "\n",
    "    # Specifiy the output file name and directory\n",
    "    outputname = 'AC9_' + str(date) + '_'+ str(site) +'_' + str(sample_type) + '_' + str(a_or_c) +'_' + str(rep)+ '_'  + 'T' + str(temp_string) + '.csv'\n",
    "    outputdir = new_dir_path+'/'+ outputname \n",
    "\n",
    "    # make a new dataframe from the summary statistics and export\n",
    "    if a_or_c == 'a':\n",
    "        output_df = pd.DataFrame([wl_a_sorted, a_mean, a_std, a_median, a_var, a_IQR]).swapaxes(0,1)\n",
    "        output_df.columns = ('wl', 'a_mean', 'a_std', 'a_median', 'a_var','a_IQR')\n",
    "        output_df.to_csv(outputdir, sep='\\t')\n",
    "    else:\n",
    "        output_df = pd.DataFrame([wl_c_sorted, c_mean, c_std, c_median, c_var, c_IQR]).swapaxes(0,1)\n",
    "        output_df.columns = ('wl', 'c_mean', 'c_std', 'c_median', 'c_var','c_IQR')\n",
    "        output_df.to_csv(outputdir, sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
