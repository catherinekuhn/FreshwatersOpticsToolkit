{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# ABOVE | AC9 Data Processing\n",
    "***\n",
    "## 01 Read AC9 files\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Authors:** Catherine Kuhn and Elena TerziÄ‡ and Anna Simpson\n",
    "**Last Updated:** August, 29th, 2018\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code reads in raw ac9 .dat files and parses sample information from the filename and header information inside the file. The output is a table of summary statistics formatted as a *.csv* file for each wavelength for each file. This code was built for a worflow in which a and c sides are sampled separately. File names should contain: date, site, rep, a or c side and water temperature. \n",
    "\n",
    "File names are formatted like ** AC9_dddddd_sit_sam_s_r_TXX_XX.dat** where:\n",
    "\n",
    "- **dddddd** = date (071718)\n",
    "- **sit** = three letter site code (fai)\n",
    "- **sam** = three letter sample type (cal, raw, fil) for calibration, raw water (unfiltered) or filtered (fil)\n",
    "- **r** = numbered replicate (1, 2, 3) \n",
    "- **TXX_X** = temperature in Celcius (T17_3)\n",
    "    \n",
    "**Ex:** AC9_071618_y17_raw_a_1_T17_6.dat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Import the required python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subdirectories(directory=os.getcwd()):\n",
    "    if directory in os.getcwd():\n",
    "        directories = os.listdir(os.getcwd())\n",
    "    else:\n",
    "        directories = os.listdir(directory)\n",
    "    directories = [subdirectory for subdirectory in directories if not subdirectory.startswith('.')]\n",
    "    return directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_subdirectories('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define directory where raw data is located\n",
    "raw_data_directory = \"1_1_renamed_originals\"\n",
    "### New directory name to store summary data\n",
    "new_dir_name = '2_summary_stats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Will be operating from Code directory - this gets parent directory path\n",
    "parent_directory = os.path.abspath('..')\n",
    "### Get path to raw data\n",
    "raw_data_file_path = parent_directory+'/Data/'+raw_data_directory\n",
    "# Get a list of all the subdirectories in the directory\n",
    "directories = os.listdir(raw_data_file_path)\n",
    "# This removes any hidden files that os.listdir is picking up from your list of subdirectories\n",
    "# (was picking up .DS_store)\n",
    "directories = [subdirectory for subdirectory in directories if not subdirectory.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AC9_071318_bai_cal_a_1_T23_8\n",
      "23.8\n",
      "AC9_071318_bai_cal_a_2_T24_1\n",
      "24.1\n",
      "AC9_071318_bai_cal_a_3_T24_0\n",
      "24.0\n",
      "AC9_071318_bai_cal_c_1_T23_7\n",
      "23.7\n",
      "AC9_071318_bai_cal_c_2_T23_8\n",
      "23.8\n",
      "AC9_071318_bai_cal_c_3_T23_9\n",
      "23.9\n",
      "AC9_071418_bai_fil_a_1_T13_4\n",
      "13.4\n",
      "AC9_071418_bai_fil_a_2_T19_2\n",
      "19.2\n",
      "AC9_071418_bai_fil_a_3_T18_5\n",
      "18.5\n",
      "AC9_071418_bai_fil_c_1_T13_4\n",
      "13.4\n",
      "AC9_071418_bai_fil_c_2_T19_2\n",
      "19.2\n",
      "AC9_071418_bai_fil_c_3_T18_5\n",
      "18.5\n",
      "AC9_071418_bai_raw_a_1_T19_8\n",
      "19.8\n",
      "AC9_071418_bai_raw_a_2_T19_8\n",
      "19.8\n",
      "AC9_071418_bai_raw_a_3_T17_8\n",
      "17.8\n",
      "AC9_071418_bai_raw_c_1_T19_8\n",
      "19.8\n",
      "AC9_071418_bai_raw_c_2_T19_8\n",
      "19.8\n",
      "AC9_071418_bai_raw_c_3_T17_8\n",
      "17.8\n",
      "AC9_071618_cbr_cal_a_1_T19_9\n",
      "19.9\n",
      "AC9_071618_cbr_cal_a_2_T19_3\n",
      "19.3\n",
      "AC9_071618_cbr_cal_a_3_T19_0\n",
      "19.0\n",
      "AC9_071618_cbr_cal_c_1_T19_9\n",
      "19.9\n",
      "AC9_071618_cbr_cal_c_2_T19_3\n",
      "19.3\n",
      "AC9_071618_cbr_cal_c_3_T19_0\n",
      "19.0\n",
      "AC9_071618_cbr_fil_a_1_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_fil_a_2_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_fil_a_3_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_fil_c_1_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_fil_c_2_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_fil_c_3_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_raw_a_1_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_raw_a_2_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_raw_a_3_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_raw_c_1_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_raw_c_2_T17_5\n",
      "17.5\n",
      "AC9_071618_cbr_raw_c_3_T17_5\n",
      "17.5\n",
      "AC9_071718_y20_fil_a_1_T17_0\n",
      "17.0\n",
      "AC9_071718_y20_fil_a_2_T19_7\n",
      "19.7\n",
      "AC9_071718_y20_fil_c_1_T17_0\n",
      "17.0\n",
      "AC9_071718_y20_fil_c_2_T19_7\n",
      "19.7\n",
      "AC9_071718_y20_raw_a_1_T16_8\n",
      "16.8\n",
      "AC9_071718_y20_raw_a_2_T16_8\n",
      "16.8\n",
      "AC9_071718_y20_raw_a_3_T16_8\n",
      "16.8\n",
      "AC9_071718_y20_raw_c_1_T16_8\n",
      "16.8\n",
      "AC9_071718_y20_raw_c_2_T16_8\n",
      "16.8\n",
      "AC9_071718_y20_raw_c_3_T16_8\n",
      "16.8\n",
      "AC9_071918_y17_fil_c_1_T18_3\n",
      "18.3\n",
      "AC9_071918_y17_fil_c_2_T18_0\n",
      "18.0\n",
      "AC9_071918_y17_fil_c_3_T17_9\n",
      "17.9\n",
      "AC9_071918_y20_fil_a_1_T17_7\n",
      "17.7\n",
      "AC9_071918_y20_fil_a_2_T17_8\n",
      "17.8\n",
      "AC9_071918_y20_fil_a_3_T17_6\n",
      "17.6\n",
      "AC9_071918_y20_fil_a_4_T17_5\n",
      "17.5\n",
      "AC9_071918_y20_fil_c_1_T17_7\n",
      "17.7\n",
      "AC9_071918_y20_fil_c_2_T17_8\n",
      "17.8\n",
      "AC9_071918_y20_fil_c_3_T17_6\n",
      "17.6\n",
      "AC9_071918_y20_fil_c_4_T17_5\n",
      "17.5\n",
      "AC9_071718_sco_fil_a_2_T17_7\n",
      "17.7\n",
      "AC9_071718_sco_fil_a_3_T17_7\n",
      "17.7\n",
      "AC9_071718_sco_fil_c_2_T17_7\n",
      "17.7\n",
      "AC9_071718_sco_fil_c_3_T17_7\n",
      "17.7\n",
      "AC9_071718_sco_raw_a_1_T18_3\n",
      "18.3\n",
      "AC9_071718_sco_raw_a_2_T18_3\n",
      "18.3\n",
      "AC9_071718_sco_raw_a_3_T18_3\n",
      "18.3\n",
      "AC9_071718_sco_raw_c_1_T18_3\n",
      "18.3\n",
      "AC9_071718_sco_raw_c_2_T18_3\n",
      "18.3\n",
      "AC9_071718_sco_raw_c_3_T18_3\n",
      "18.3\n",
      "AC9_071718_9mi_cal_a_1_T19_0\n",
      "19.0\n",
      "AC9_071718_9mi_cal_c_1_T19_0\n",
      "19.0\n",
      "AC9_071718_9mi_fil_a_1_T15_4\n",
      "15.4\n",
      "AC9_071718_9mi_fil_a_2_T15_4\n",
      "15.4\n",
      "AC9_071718_9mi_fil_a_3_T15_1\n",
      "15.1\n",
      "AC9_071718_9mi_fil_c_1_T15_4\n",
      "15.4\n",
      "AC9_071718_9mi_fil_c_2_T15_4\n",
      "15.4\n",
      "AC9_071718_9mi_fil_c_3_T15_1\n",
      "15.1\n",
      "AC9_071718_9mi_raw_a_1_T17_4\n",
      "17.4\n",
      "AC9_071718_9mi_raw_a_2_T17_4\n",
      "17.4\n",
      "AC9_071718_9mi_raw_a_3_T17_4\n",
      "17.4\n",
      "AC9_071718_9mi_raw_c_1_T17_4\n",
      "17.4\n",
      "AC9_071718_9mi_raw_c_2_T17_4\n",
      "17.4\n",
      "AC9_071718_9mi_raw_c_3_T17_4\n",
      "17.4\n",
      "AC9_071718_y17_fil_a_1_T17_4\n",
      "17.4\n",
      "AC9_071718_y17_fil_a_2_T17_4\n",
      "17.4\n",
      "AC9_071718_y17_fil_c_1_T17_4\n",
      "17.4\n",
      "AC9_071718_y17_fil_c_2_T17_4\n",
      "17.4\n",
      "AC9_071718_y17_raw_a_2_T17_1\n",
      "17.1\n",
      "AC9_071718_y17_raw_a_3_T17_1\n",
      "17.1\n",
      "AC9_071718_y17_raw_a_4_T17_1\n",
      "17.1\n",
      "AC9_071718_y17_raw_c_2_T17_1\n",
      "17.1\n",
      "AC9_071718_y17_raw_c_3_T17_1\n",
      "17.1\n",
      "AC9_071718_y17_raw_c_4_T17_1\n",
      "17.1\n",
      "AC9_071918_y17_fil_a_1_T18_3\n",
      "18.3\n",
      "AC9_071918_y17_fil_a_2_T18_0\n",
      "18.0\n",
      "AC9_071918_y17_fil_a_3_T17_9\n",
      "17.9\n",
      "AC9_071918_y17_fil_c_1_T18_3\n",
      "18.3\n",
      "AC9_071918_y17_fil_c_2_T18_0\n",
      "18.0\n",
      "AC9_071918_y17_fil_c_3_T17_9\n",
      "17.9\n",
      "AC9_071818_boo_fil_a_1_T22_3\n",
      "22.3\n",
      "AC9_071818_boo_fil_a_2_T22_3\n",
      "22.3\n",
      "AC9_071818_boo_fil_a_3_T21_9\n",
      "21.9\n",
      "AC9_071818_boo_fil_c_1_T22_3\n",
      "22.3\n",
      "AC9_071818_boo_fil_c_2_T22_3\n",
      "22.3\n",
      "AC9_071818_boo_fil_c_3_T21_9\n",
      "21.9\n",
      "AC9_071818_boo_raw_a_1_T17_0\n",
      "17.0\n",
      "AC9_071818_boo_raw_a_2_T17_0\n",
      "17.0\n",
      "AC9_071818_boo_raw_a_3_T17_0\n",
      "17.0\n",
      "AC9_071818_boo_raw_c_1_T17_0\n",
      "17.0\n",
      "AC9_071818_boo_raw_c_2_T17_0\n",
      "17.0\n",
      "AC9_071818_boo_raw_c_3_T17_0\n",
      "17.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c7874cf4099e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbasename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mSensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSample_Type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAbs_Type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT2\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'T'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mT2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "date = []\n",
    "site = []\n",
    "sample_type = []\n",
    "abs_type = []\n",
    "rep = []\n",
    "temp = []\n",
    "\n",
    "for i in get_subdirectories(raw_data_file_path):\n",
    "    file_list = sorted(glob.glob(raw_data_file_path+'/'+i+'/*.dat'))\n",
    "    for j in file_list:\n",
    "        basename = j.split('/')[-1].split('.')[0]\n",
    "        Sensor, Date, Site, Sample_Type, Abs_Type, Rep, T1, T2= basename.split('_')\n",
    "        T = float(T1.lstrip('T')+'.'+T2)\n",
    "        date.append(Date)\n",
    "        site.append(Site)\n",
    "        sample_type.append(Sample_Type)\n",
    "        abs_type.append(Abs_Type)\n",
    "        \n",
    "        print basename\n",
    "        print T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary  storing subdirectory/site names (keys) \n",
    "# and lists of files within each subdirectory (entries)\n",
    "site_files_dict = {}\n",
    "# Iterate through subdirectories\n",
    "for i in directories:\n",
    "    path=raw_data_file_path+'/'+i+'/'\n",
    "    # Get a list of the files in each subdirectory as the entry for each site name key\n",
    "    site_files_dict[i] = sorted(glob.glob(path+'*.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define the new directory where summary stats will be stored\n",
    "\n",
    "#### Default action: summary stat files will be stored in subdirectories (defined by site names) in the new directory you have defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Get full path name of new directory\n",
    "new_dir_path = os.path.abspath('..')+'/Data/'+new_dir_name\n",
    "\n",
    "## Generate new directory if it doesn't already exist\n",
    "if not os.path.exists(new_dir_path):\n",
    "    os.makedirs(new_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate summary statistics and make new csv files for each sample\n",
    "\n",
    "#### Use the columns in the upper part of the document for the wavelengths. Just to organize your main dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:63: FutureWarning: convert_objects is deprecated.  Use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    }
   ],
   "source": [
    "# iterate through dictionary key entries (site names)\n",
    "for site_name in site_files_dict:\n",
    "    # define the path for each new subdirectory that will be made based on site name\n",
    "    individual_site_directory_path = new_dir_path+'/'+site_name\n",
    "    # generate the new subdirectory for each site if it doesn't already exist\n",
    "    if not os.path.exists(individual_site_directory_path):\n",
    "        os.makedirs(individual_site_directory_path)\n",
    "    # Get the list of original raw files for this site\n",
    "    site_files = site_files_dict[site_name]\n",
    "    # Iterate through that list of files so that:\n",
    "    # each file can be imported ad each summary stat file can be generated\n",
    "    for file in range(len(site_files)):\n",
    "    # brings in file\n",
    "        filename = site_files[file].split('/')[-1]\n",
    "        read_wl = pd.read_csv(site_files[file], skiprows=10, names=range(100), delimiter= '\\t')  # use names= range (100) to clip dangling columns\n",
    "        # reads in a and c wavelength values from the first column of data\n",
    "        a_c_wl = read_wl[0][0:18]  ; a_wl = read_wl[0][0:9] ; c_wl = read_wl[0][9:18];\n",
    "\n",
    "        # parses temperature, samplename, a or c side and site from the file name\n",
    "        # 0 = A, 0:3 takes AC9 (letters 0, 1, 2 not 3)\n",
    "        temperature = float(filename[24:28].replace('_', '.'))\n",
    "        temp_string = filename[24:28]; sample_type = filename[15:18];  a_or_c = filename[19:20];   \n",
    "        date = filename[4:10];         site = filename[11:14];   rep = filename[21:22];\n",
    "\n",
    "        # make empty objects for your new variables of the wavelength value and name\n",
    "        # Example: wl_a: 650.0; wl_a_str: a650\n",
    "        wl_a = []    ; wl_c = []  ; wl_a_str = []  ; wl_c_str = []\n",
    "\n",
    "        # makes a list of the 9 wavelengths formatted as floats\n",
    "        for i in range(len(a_wl)):\n",
    "            wl_a.append(np.float(a_wl[i][1:4]))\n",
    "            wl_a_str.append(a_wl[i])\n",
    "        for j in range(len(c_wl)):\n",
    "            wl_c.append(np.float(c_wl[j+9][1:4]))\n",
    "            wl_c_str.append(c_wl[j+9])\n",
    "\n",
    "         # Unsorted list of wavelengths (412) and wavelength strings (a676)\n",
    "        wavelist = wl_a + wl_c                   ; wavelist_str = wl_a_str + wl_c_str   \n",
    "        # Unsorted list of wavelengths (412) and wavelength strings (a676) as arrays\n",
    "        wavelengths = np.asarray(wavelist)       ; wavelengths_str = np.asarray(wavelist_str)  \n",
    "        # Sorted list of a and c wavelengths as floats in an array (ex: 412, 440, etc)\n",
    "        wl_a_sorted = np.asarray(sorted(wl_a))   ; wl_c_sorted = np.asarray(sorted(wl_c))\n",
    "\n",
    "        # Now read back in the data, skipping all the header information  \n",
    "        # The time series of measured values starts in the 32th row\n",
    "        df1 = pd.read_csv(site_files[file], skiprows=31, delimiter= '\\t') \n",
    "\n",
    "        # drops all the ragged extra columns dangling to the right\n",
    "        columns = df1.columns[19:len(df1.columns)]                 \n",
    "        df2 = df1.drop(columns, axis=1)    # you should have 19 cols left ~ array size [ntimesteps, 19]                       \n",
    "\n",
    "        # drops the first column of the timestamp (ntimesteps)\n",
    "        columns1 = df2.columns[0]\n",
    "        df3 = df2.drop(columns1, axis=1)                    \n",
    "\n",
    "        # makes a new header from the list of wavelengths you parsed earlier  \n",
    "        wl_header = wavelengths_str \n",
    "\n",
    "        # Clean and reindex\n",
    "        df4 = df3[1:]                                       # take the data (row 1- n) less the header row (row 0)\n",
    "        df4.columns = wavelengths_str                       # set the header row as list of wavelengths\n",
    "        df4 = df4.reindex_axis(sorted(df4.columns), axis=1) # reindex them by the new sorted wavelengths\n",
    "        df4=df4.convert_objects(convert_numeric=True)       # Just to make sure that all elements are floats!\n",
    "\n",
    "        no_cols = int(len(df4.columns)/2.)                  # no_col should always be 9 (one for each wavelength)         \n",
    "\n",
    "        # Sort your dataframe with ascending walues of your wavelengths\n",
    "        # at this point the wl_a and wl_c are the same wavelengths so \n",
    "        # it doesn't matter which one you use here\n",
    "        new_header = wl_a_sorted\n",
    "\n",
    "        # reindex to reshape the data\n",
    "        df_a_aux = df4.iloc[:, :no_cols];  df_a_aux.columns = new_header ;  df_a = df_a_aux.reindex_axis(sorted(df_a_aux.columns), axis = 1)\n",
    "        df_c_aux = df4.iloc[:, no_cols:];  df_c_aux.columns = new_header ;  df_c = df_c_aux.reindex_axis(sorted(df_c_aux.columns), axis = 1)\n",
    "\n",
    "        # calculate the me(di)an, stdev, IQR for the time series - per each column\n",
    "        a_mean = df_a[wl_a_sorted].mean(axis=0)         ; c_mean = df_c[wl_c_sorted].mean(axis=0)   \n",
    "        a_std  = df_a[wl_a_sorted].std(axis=0)          ; c_std  = df_c[wl_c_sorted].std(axis=0)\n",
    "        a_median = df_a[wl_a_sorted].median(axis=0)     ; c_median = df_c[wl_c_sorted].median(axis=0)\n",
    "\n",
    "        # Computing IQR\n",
    "        a_Q1 = df_a[wl_a_sorted].quantile(0.25)         ; c_Q1 = df_c[wl_c_sorted].quantile(0.25)\n",
    "        a_Q3 = df_a[wl_a_sorted].quantile(0.75)         ; c_Q3 = df_c[wl_c_sorted].quantile(0.75)\n",
    "        a_IQR = a_Q3 - a_Q1                             ; c_IQR = c_Q3 - c_Q1\n",
    "\n",
    "        # Specifiy the output file name and directory\n",
    "        outputname = 'AC9_' + str(date) + '_'+ str(site) +'_' + str(sample_type) + '_' + str(a_or_c) +'_' + str(rep)+ '_'  + 'T' + str(temp_string) + '.csv'\n",
    "        outputdir = new_dir_path+'/'+site_name+'/'+ outputname \n",
    "\n",
    "        # make a new dataframe from the summary statistics and export\n",
    "        if a_or_c == 'a':\n",
    "            output_df = pd.DataFrame([wl_a_sorted, a_mean, a_std, a_median, a_IQR]).swapaxes(0,1)\n",
    "            output_df.columns = ('wl', 'a_mean', 'a_std', 'a_median', 'a_IQR')\n",
    "            output_df.to_csv(outputdir, sep='\\t')\n",
    "        else:\n",
    "            output_df = pd.DataFrame([wl_c_sorted, c_mean, c_std, c_median, c_IQR]).swapaxes(0,1)\n",
    "            output_df.columns = ('wl', 'c_mean', 'c_std', 'c_median', 'c_IQR')\n",
    "            output_df.to_csv(outputdir, sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
