{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOVE AC9 Data Processing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter correction and calculating particulate backscatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fopt_toolkit import fopt_toolkit as fp\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get path to directory containing particulate (vicariously calibrated) absorption, pre-scatter correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "particulate_absorption_filepath = fp.make_dir('Data/4_vic_averaged_absorption_PRE_SCATTER_CORRECTION/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using path, get list of filepaths to csvs of pre-scatter-correction absorption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "particulate_absorption_file_list = sorted(glob.glob(particulate_absorption_filepath+'/*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary where keys are site names (lakes) and entries are dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_absorption_dfs = {i.split('/')[-1].split('_')[1]:pd.read_csv(i, skiprows=0, delimiter= '\\t', index_col=0) \n",
    "               for i in particulate_absorption_file_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now you have a dictionary of dataframes of particulate absorption, with the keys as lake names and the entries as dataframes. Do with it what you will. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving corrected particulate absorption files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now these are just passing along the uncorrected dataframes, since scatter correction is not present yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_particulate_path = fp.make_dir('Data/5_final_vical/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dict_of_absorption_dfs:\n",
    "    df = dict_of_absorption_dfs[i].reset_index()\n",
    "    filename = final_particulate_path+'Vical_'+i+'_a.csv'\n",
    "    df.to_csv(filename, sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating backscatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to final particulate files (from whence we shall pull final particulate absorption and attenuation, and save final particulate scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_particulate_path = fp.make_dir('Data/5_final_vical/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of all filepaths to final particulate absorption and attenuation csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "particulate_final_file_list = sorted(glob.glob(final_particulate_path+'/*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list = list(set([i.split('/')[-1].split('_')[1] for i in particulate_final_file_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty nested dictionary to store dataframes, so that each site is the key to another dictionary, where the keys are 'a' and 'c' and the entries are the corresponding dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_to_df_dict = {i:{'a':None, 'c':None} for i in site_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill dictionary (now we can call each site and get both a and c dataframes at once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in particulate_final_file_list:\n",
    "    file_name_elements = file_path.split('/')[-1].split('_')\n",
    "    site = file_name_elements[1]\n",
    "    analysis_type = file_name_elements[2].split('.')[0]\n",
    "    df = pd.read_csv(file_path, skiprows=0, delimiter= '\\t', index_col=0)\n",
    "    site_to_df_dict[site][analysis_type] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to combine standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_std(series):\n",
    "    from math import sqrt\n",
    "    return sqrt(sum([i**2 for i in series]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate scatter (b) and save to file containing final particulate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in site_to_df_dict:\n",
    "    a = site_to_df_dict[site]['a']\n",
    "    c = site_to_df_dict[site]['c']\n",
    "    b = c.copy(deep=True)\n",
    "    b['mean'] = c['mean']-a['mean']\n",
    "    b['std'] = pd.concat([a['std'], c['std']]).groupby(level=0).agg(combine_std)\n",
    "    b.rename_axis(None, axis=1)\n",
    "    filename = 'Vical_'+site+'_b.csv'\n",
    "    b.to_csv(final_particulate_path+filename, sep='\\t',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
